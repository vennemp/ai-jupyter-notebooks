{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will examining how to chain outputs from one prompt to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import boto3\n",
    "\n",
    "REGION_NAME=\"us-east-1\" ## change to your region\n",
    "PROFILE_NAME=\"lza-comm-gss\"  ## change to your desired aws credential profile\n",
    "## ensure Anthropic Claude is enabled in your AWS Account.\n",
    "named_profile = boto3.session.Session(profile_name=PROFILE_NAME)\n",
    "bedrock_client = named_profile.client('bedrock-runtime')\n",
    "print('Initalizing Anthropic Claude v2')\n",
    "model = Bedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    endpoint_url=\"https://bedrock-runtime.\" + REGION_NAME + \".amazonaws.com\",\n",
    "    model_kwargs={\"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs often need specific instructions and structure in order to reason through a task.  The more complex the task, the more structure it needs. \n",
    "\n",
    "### Chains\n",
    "In this task we will be asking Bedrock - given a type of food, what country is famous for that food? what is the capital of that country (which requires the output from first question) and what is a recipe for that food (recalling the original input)?\n",
    "\n",
    "Notice the two separate prompts one to answer the first part - the country, which will pass down to the second prompt. \n",
    "\n",
    "[We use LCEL to construct the chain1 for the first prompt.](https://python.langchain.com/docs/expression_language/get_started)\n",
    "\n",
    "In the second chain, notice the second prompt has two inputs - country and food - so we must pass them in a dictionary - country must call the output of the first chain, and then we must use the item getter to get the original input.\n",
    "\n",
    "### Model differences\n",
    "Notice we must define a model for each chain - it's important to know that different models are better at different tasks - like text generation vs image generation.  We will explore this in a different lab.  In this lab, we will use Anthropic for both.\n",
    "\n",
    "We then invoke chain 2 (which calls chain1), and pass it a dictionary with food as the key and whatever food we want to test this with as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what country is famous for {food} ?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what the capital city of {country} in? provide a recipe for {food}\"\n",
    ")\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"food\": itemgetter(\"food\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain2.invoke({\"food\": \"pizza\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
