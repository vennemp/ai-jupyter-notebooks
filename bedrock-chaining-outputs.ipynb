{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will examining how to chain outputs from one prompt to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import boto3\n",
    "\n",
    "REGION_NAME=\"us-east-1\" ## change to your region\n",
    "PROFILE_NAME=\"lza-comm-gss\"  ## change to your desired aws credential profile\n",
    "## ensure Anthropic Claude is enabled in your AWS Account.\n",
    "named_profile = boto3.session.Session(profile_name=PROFILE_NAME)\n",
    "bedrock_client = named_profile.client('bedrock-runtime')\n",
    "print('Initalizing Anthropic Claude v2')\n",
    "model = Bedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    endpoint_url=\"https://bedrock-runtime.\" + REGION_NAME + \".amazonaws.com\",\n",
    "    model_kwargs={\"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs often need specific instructions and structure in order to reason through a task.  The more complex the task, the more structure it needs. \n",
    "\n",
    "### Chains\n",
    "In this task we will be asking Bedrock - given a type of food, what country is famous for that food? what is the capital of that country (which requires the output from first question) and what is a recipe for that food (recalling the original input)?\n",
    "\n",
    "Notice the two separate prompts one to answer the first part - the country, which will pass down to the second prompt. \n",
    "\n",
    "[We use LCEL to construct the chain1 for the first prompt.](https://python.langchain.com/docs/expression_language/get_started)\n",
    "\n",
    "In the second chain, notice the second prompt has two inputs - country and food - so we must pass them in a dictionary - country must call the output of the first chain, and then we must use the item getter to get the original input.\n",
    "\n",
    "### Model differences\n",
    "Notice we must define a model for each chain - it's important to know that different models are better at different tasks - like text generation vs image generation.  We will explore this in a different lab.  In this lab, we will use Anthropic for both.\n",
    "\n",
    "We then invoke chain 2 (which calls chain1), and pass it a dictionary with food as the key and whatever food we want to test this with as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what country is famous for {food} ?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what the capital city of {country} in? provide a recipe for {food}\"\n",
    ")\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"food\": itemgetter(\"food\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain2.invoke({\"food\": \"pizza\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something a little more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"generate a {attribute} color. Return the name of the color and nothing else. .\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else.\"\n",
    ")\n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
    ")\n",
    "prompt4 = ChatPromptTemplate.from_template(\n",
    "   #\"Answer all 3 of these questions succinctly. 1. What is the color of {fruit}? 2. What is the color of the flag of {country}? 3. What is a synonym of {attribute}?\"\n",
    "    \"What is the color of {fruit} and the flag of {country}?\"\n",
    ")\n",
    "\n",
    "model_parser = model | StrOutputParser()\n",
    "color_to_fruit = prompt2 | model_parser\n",
    "color_to_country = prompt3 | model_parser\n",
    "color_generator = (\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
    ")\n",
    "question_generator = (\n",
    "   color_generator| {\"fruit\": color_to_fruit, \"country\": color_to_country, \"attribute\": RunnablePassthrough() } | prompt4\n",
    ")\n",
    "prompt = question_generator.invoke(\"warm\")\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break this down. \n",
    "\n",
    "There's a lot going on here so let's think about logically what the LLM is doing.\n",
    "\n",
    "The final question we want to ask is: given a fruit and a country - what is the color of the fruit and the color of the flag of that country?\n",
    "\n",
    "Looking at the input we pass the model - we give an a attribute \"warm\"... so how do does it go from a random attribue to fruit and country?\n",
    "\n",
    "## Prompt #1\n",
    "\n",
    "We see that the first prompt - takes an attribute and generates a color with that attribute.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"generate a {attribute} color. Return the name of the color and nothing else. .\"\n",
    ")\n",
    "```\n",
    "\n",
    "Try it below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_generator = (\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
    ")\n",
    "color_generator.invoke({\"attribute\": \"warm\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes the attribute warm and it returns orange.  Try it with different adjectives.\n",
    "\n",
    "Note that the input is passed as a RunnablePassthrough - this essentially just means - return the  initial argument that is passed to this sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "If we print out the type of color_to_country object - it is a runnable sequence, where the output of one object is the input of the next.  Color_generator and color_to_fruit are both runnable sequences which is why we can pipe to one another. \n",
    "\n",
    "```\n",
    "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
    "```\n",
    "Printing the value we get - \n",
    "```\n",
    "first=ChatPromptTemplate(input_variables=['color'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['color'], template='what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:'))]) last=Bedrock(client=<botocore.client.BedrockRuntime object at 0x120554f10>, model_id='anthropic.claude-v2', model_kwargs={'temperature': 0}, endpoint_url='https://bedrock-runtime.us-east-1.amazonaws.com')\n",
    "| StrOutputParser()\n",
    "```\n",
    "\n",
    "If we invoke - this with sequence individually - we get a response to what country has a flag of color the given color?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(color_to_country.invoke({\"color\": \"red\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt #2\n",
    "\n",
    "The second prompt takes a color and returns a fruit of that color.  Which is SUPER convenient since the output of the first prompt is a color!\n",
    "\n",
    "```python\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else.\"\n",
    ")\n",
    "```\n",
    "\n",
    "So for prompt 2 to perform, it takes the attribute input pass it to the first prompt, and then take that output and pass it to the second prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fruit_generator= color_generator | {\"color\": color_to_fruit} | prompt2|{\"fruit\": model_parser}\n",
    "\n",
    "fruit_generator.invoke({\"attribute\": \"warm\"})\n",
    "# model.invoke(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt #3\n",
    "\n",
    "Prompt # 3 takes a color and returns a country with a flag that has a flag of that color.\n",
    "\n",
    "```\n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
    ")\n",
    "```\n",
    "\n",
    "We use similar logic as the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "country_generator= color_generator | {\"color\": color_to_country} | prompt3|{\"country\": model_parser}\n",
    "\n",
    "country_generator.invoke({\"attribute\": \"warm\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer may be a little silly - for example, when testing this, I used \"warm\" and it returned Orange in the Prompt #1 output and netherlands for Prompt #3.  Netherlands does not have an orange flag... but let's pretend this is correct for now.  This is a good example of issues that can override on over-reliance of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's string it all together.  \n",
    "\n",
    "We want to construct our final prompt to which we will actually pass our LLM - \n",
    "\n",
    "Prompt #2 and Prompt #3 rely on color_generator so we use the result of that as the input for the chain.  In the next chain we define a dictionary - \n",
    "\n",
    "```json\n",
    "{\"fruit\": color_to_fruit,\n",
    " \"country\": color_to_country }\n",
    "```\n",
    "\n",
    "notice the keys have same name as the attributes used in prompt 4. \n",
    "\n",
    "and the values are the sequences we defined to render the values using our LLM and our prompts.\n",
    "\n",
    "## Finally\n",
    "\n",
    "Running the below cell will render the final question that we will forward to our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = (\n",
    "   color_generator| {\"fruit\": color_to_fruit, \"country\": color_to_country } | prompt4\n",
    ")\n",
    "\n",
    "prompt = question_generator.invoke(\"warm\")\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And invoke.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this give us some understanding as to why the LLM said Netherlands has an orange flag - it is because it thinks the fruit orange is reddish-yellow.  It uses red to determine the flag - which has red in it.  A situation like this would require more prompt engineering to rectify - but that is for a different time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
