{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs by default wait until they have their entire response before returning it to the user.  \n",
    "\n",
    "In this lab we will explore streaming - where the response is returned to the user in chunks as they are generated.  This typically gives a better User Experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Anthropic Claude in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_NAME=\"us-east-1\" ## change to your region\n",
    "PROFILE_NAME=\"lza-comm-gss\"  ## change to your desired aws credential profile\n",
    "## ensure AI21 Jurassic Ultra is enabled in your AWS Account.\n",
    "named_profile = boto3.session.Session(profile_name=PROFILE_NAME)\n",
    "bedrock_client = named_profile.client('bedrock-runtime')\n",
    "print('Initalizing Anthropic Claude v2')\n",
    "model = Bedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    endpoint_url=\"https://bedrock-runtime.\" + REGION_NAME + \".amazonaws.com\",\n",
    "    model_kwargs={\"temperature\": 0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are using a template in this example.  \n",
    "\n",
    "Templates allow parameterizing input so you can inject context.   This template uses a chat structure where we tell the system to act as a knowledgeable historian.  Then we can pass it a question - where we can inject the user input in the {}.  \n",
    "\n",
    "Also notice, the chat template requires a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Notice that we are using the runnable construct - this is LCEL - Langchain Expression Language - which helps simply defining components.  \n",
    "\n",
    "Also notice the outputp arser at this end.  Output parsers are used to set the type and structure of your output.  In this example the StrOutputParser is redundant since it already returns a stream  - but feel free to play around with different parsers and see your results.\n",
    "\n",
    "### Further Reading\n",
    "[Output Parsers ](https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model | StrOutputParser()\n",
    "question={\"question\":\"Who was the first president of the United States?\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we invoke our model.  Notice we loop throught he stream and print each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for chunk in runnable.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
